---
id: 83
title: 'DISTRIBUTED SYSTEMS: LECTURE 3'
date: 2015-11-06T11:47:00+00:00
author: chito
layout: post
guid: http://www.afriqueunique.org/2015/11/06/11942289/
permalink: /2015/11/06/11942289/
swp_pinterest_image_url:
  - ""
swp_cache_timestamp:
  - "419230"
post_views_count:
  - "155"
bs_social_share_facebook:
  - "0"
bs_social_share_twitter:
  - "0"
bs_social_share_reddit:
  - "0"
bs_social_share_google_plus:
  - "0"
bs_social_share_linkedin:
  - "0"
bs_social_share_interval:
  - "1568145600"
categories:
  - LEARNING
---
<span style="font-size:small;">Distributed Systems Fö 3 &#8211; 1<br />Petru Eles, IDA, LiTH<br /><strong>COMMUNICATION IN DISTRIBUTED</strong><br /><strong>SYSTEMS</strong><br /><strong>1. Communication System: Layered Implementation</strong><br /><strong>2. Network Protocol</strong><br /><strong>3. Request and Reply Primitives</strong><br /><strong>4. RMI and RPC</strong><br /><strong>5. RMI and RPC Semantics and Failures</strong><br /><strong>6. Indirect Communication</strong><br /><strong>7. Group Communication</strong><br /><strong>8. Publish-Subscribe Systems</strong><br />Distributed Systems Fö 3 &#8211; 2<br />Petru Eles, IDA, LiTH<br /><strong>Communication Models and their Layered</strong><br /><strong>Implementation</strong><br />• This chapter concentrates on communication<br />between distributed objects by means of two<br />models: <strong>remote method invocation</strong> (<strong>RMI</strong>) and<br /><strong>remote procedure call</strong> (<strong>RPI</strong>).<br />• RMI, as well as RPC, are based on <em>request</em> and<br /><em>reply</em> primitives.<br />• <em>Request</em> and <em>reply</em> are implemented based on the<br />network protocol (e.g. TCP or UDP in case of the<br />Internet).<br />Applications & Services<br />RMI, RPC<br />Operating System&Network Protocol<br />Hardware: Computer&Network<br />Request&Reply<br />Middleware<br />Distributed Systems Fö 3 &#8211; 3<br />Petru Eles, IDA, LiTH<br /><strong>Network Protocol</strong><br />☞ Middleware and distributed applications have to be<br />implemented on top of a network protocol. Such a<br />protocol is implemented as several layers.<br />In case of the Internet:<br />• TCP (Transport Control Protocol) and UDP (User<br />Datagram Protocol) are both transport protocols<br />implemented on top of the Internet protocol (IP).<br />Applications & Services<br />TCP or UDP<br />Middleware<br />IP<br />lower level layers<br />Distributed Systems Fö 3 &#8211; 4<br />Petru Eles, IDA, LiTH<br /><strong>Network Protocol (cont’d)</strong><br />☞ TCP is a reliable protocol.<br />• TCP guarantees the delivery to the receiving<br />process of all data delivered by the sending<br />process, in the same order.<br />• TCP implements additional mechanisms on top of<br />IP to meet reliability guarantees.<br />&#8211; Sequencing:<br />A sequence number is attached to each<br />transmitted segment (packet). At the receiver<br />side, no segment is delivered until all lowernumbered segments have been delivered.<br />&#8211; Flow control:<br />The sender takes care not to overwhelm the<br />receiver (or intermediate nodes). This is based<br />on periodic acknowledgements received by the<br />sender from the receiver.<br />&#8211; Retransmission and duplicate handling:<br />If a segment is not acknowledged within a<br />specified timeout, the sender retransmits it.<br />Based on the sequence number, the receiver is<br />able to detect and reject duplicates.<br />&#8211; Buffering:<br />Buffering is used to balance the flow between<br />sender and receiver. If the receiving buffer is<br />full, incoming segments are dropped. They will<br />not be acknowledged and the sender will<br />retransmit them.<br />&#8211; Checksum:<br />Each segment carries a checksum. If the<br />received segment doesn’t match the checksum,<br />it is dropped (and will be retransmitted)<br />Distributed Systems Fö 3 &#8211; 5<br />Petru Eles, IDA, LiTH<br /><strong>Network Protocol (cont’d)</strong><br />☞ UDP is a protocol that does not guarantee reliable<br />transmission.<br />• UDP offers no guarantee of delivery.<br />According to the IP, packets may be dropped<br />because of congestion or network error. UDP adds<br />no additional reliability mechanism to this.<br />• UDP provides a means of transmitting messages<br />with minimal additional costs or transmission delays<br />above those due to IP transmission.<br />Its use is restricted to applications and services that<br />do not require reliable delivery of messages.<br />• If reliable delivery is requested with UDP, reliability<br />mechanisms have to be implemented at the<br />application level.<br />Distributed Systems Fö 3 &#8211; 6<br />Petru Eles, IDA, LiTH<br /><strong>Request and Reply Primitives</strong><br />☞ Communication between processes and objects in a<br />distributed system is performed by message passing.<br />• In a typical scenario (e.g. client-server model) such<br />a communication is through request and reply<br />messages.<br />Distributed Systems Fö 3 &#8211; 7<br />Petru Eles, IDA, LiTH<br /><strong>Request-Reply Communication in a</strong><br /><strong>Client-Server Model</strong><br />The system is structured as a group of processes<br />(objects), called <em>servers</em>, that deliver services to <em>clients</em>.<br />Client<br />Network<br />Request<br />Reply<br />The client:<br />&#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211;<br />send (request) to server_reference;<br />receive(reply);<br />&#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211;<br />The server:<br />&#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211;<br />receive(request) from client-reference;<br /><em>execute requested operation</em><br />send (reply) to client_reference;<br />&#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211;<br />Server<br />Distributed Systems Fö 3 &#8211; 8<br />Petru Eles, IDA, LiTH<br /><strong>Remote Method Invocation (RMI) and</strong><br /><strong>Remote Procedure Call (RPC)</strong><br />The goal: make, for the programmer, distributed<br />computing look like centralized computing.<br />The solution:<br />&#8211; Asking for a service is solved by the client<br />issuing a simple <em>method invocation</em> or <em>procedure</em><br /><em>call</em>; because the server can be on a remote<br />machine this is a <em>remote</em> invocation (call).<br />&#8211; <em>RMI (RPC) is transparent</em>: the calling object<br />(procedure) is not aware that the called one is<br />executing on a different machine, and vice<br />versa.<br />Applications & Services<br />RMI, RPC<br />Operating System&Network Protocol<br />Hardware: Computer&Network<br />Request&Reply<br />Middleware<br />Distributed Systems Fö 3 &#8211; 9<br />Petru Eles, IDA, LiTH<br /><strong>Remote Method Invocation</strong><br />• The programmer is unaware of the request and<br />reply messages which are sent over the network<br />during execution of the RMI.<br />Network<br />Request<br />Reply<br />The client writes:<br />&#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211;<br />server_id.service(values_to_server, result_arguments);<br />&#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211; &#8211;<br />The server contains the method:<br /><strong>public</strong> service(<strong>in</strong> type1 arg_from_client; <strong>out</strong> type2 arg_to_client)<br />{ &#8211; &#8211; &#8211; };<br />Client Server<br />Distributed Systems Fö 3 &#8211; 10<br />Petru Eles, IDA, LiTH<br /><strong>Client</strong><br />Object A Proxy for B<br />Remote reference<br />module<br />Communication<br />module<br />Invocation<br />Answer<br /><strong>Server</strong><br />Skeleton for B Object B<br />Invocation<br />Answer<br />Request<br />Reply<br />Remote reference<br />module<br />Communication<br />module<br />Messages over network<br /><strong>Implementation of RMI</strong><br />Marshal<br />arguments<br />Unmarshal<br />results<br />Unmarshal<br />arguments<br />Marshal<br />results<br /><em>local ref.</em> ↔ <em>rem. ref.</em><br /><em>local ref.</em><br /><em>local ref.</em><br /><em>local ref.</em><br /><em>local ref.</em><br /><em>local ref.</em><br /><em>local ref.</em><br /><em>local ref.</em><br /><em>local ref.</em><br /><em>rem. ref.</em><br /><em>rem. ref.</em><br /><em>local ref.</em> ↔ <em>rem. ref.</em><br />Distributed Systems Fö 3 &#8211; 11<br />Petru Eles, IDA, LiTH<br /><strong>Implementation of RMI (cont’d)</strong><br />Who are the players?<br />• Object A asks for a service<br />• Object B delivers the service<br />Who more?<br />• The proxy for object B<br />&#8211; If an object A holds a remote reference to a<br />(remote) object B, there exists a proxy object for<br />B on the machine which hosts A. The proxy is<br />created when the remote object reference is<br />used for the first time. For each method in B<br />there exists a corresponding method in the proxy.<br />&#8211; The proxy is the local representative of the<br />remote object ⇒ the remote invocation from A<br />to B is initially handled like a local one from A to<br />the proxy for B.<br />&#8211; At invocation, the corresponding proxy method<br /><em>marshals</em> the arguments and builds the message<br />to be sent, as a request, to the server.<br />After reception of the reply, the proxy<br /><em>unmarshals</em> the received message and sends<br />the results, in an answer, to the invoker.<br />Distributed Systems Fö 3 &#8211; 12<br />Petru Eles, IDA, LiTH<br /><strong>Implementation of RMI (cont’d)</strong><br />• The skeleton for object B<br />&#8211; On the server side, there exists a skeleton<br />object corresponding to a class, if an object of<br />that class can be accessed by RMI. For each<br />method in B there exists a corresponding<br />method in the skeleton.<br />&#8211; The skeleton receives the request message,<br />unmarshals it and invokes the corresponding<br />method in the remote object; it waits for the<br />result and marshals it into the message to be<br />sent with the reply.<br />&#8211; A part of the skeleton is also called <em>dispatcher</em>.<br />The dispatcher receives a request from the<br /><em>communication module</em>, identifies the invoked<br />method and directs the request to the<br />corresponding method of the skeleton.<br />Distributed Systems Fö 3 &#8211; 13<br />Petru Eles, IDA, LiTH<br /><strong>Implementation of RMI (cont’d)</strong><br />• Communication module<br />&#8211; The communication modules on the client and<br />server are responsible of carrying out the<br />exchange of messages which implement the<br />request/reply protocol needed to execute the<br />remote invocation.<br />&#8211; The particular messages exchanged and the<br />way errors are handled, depends on the RMI<br />semantics which is implemented (see slide 18).<br />• Remote reference module<br />&#8211; The remote reference module translates<br />between local and remote object references.<br />The correspondence between them is recorded<br />in a <em>remote object table</em>.<br />&#8211; Remote object references are initially obtained<br />by a client from a so called <em>binder</em> that is part of<br />the global name service (it is not part of the<br />remote reference module). Here servers<br />register their remote objects and clients look up<br />after services.<br />Distributed Systems Fö 3 &#8211; 14<br />Petru Eles, IDA, LiTH<br /><strong>Implementation of RMI (cont’d)</strong><br />☞ Question 1<br />What if the two computers use different<br />representation for data (integers, chars, floating<br />point)?<br />• The most elegant and flexible solution is to have a<br />standard representation used for all values sent<br />through the network; the proxy and skeleton<br />convert to/from this representation during<br />marshalling/unmarshalling.<br />☞ Question 2<br />Who generates the classes for proxy and skeleton?<br />• In advanced middleware systems (e.g. CORBA) the<br />classes for proxies and skeletons can be generated<br />automatically.<br />Given the specification of the server interface and<br />the standard representations, an interface compiler<br />can generate the classes for proxies and skeletons.<br />Distributed Systems Fö 3 &#8211; 15<br />Petru Eles, IDA, LiTH<br /><strong>Implementation of RMI (cont’d)</strong><br />☞ Object A and Object B belong to the application.<br />☞ Remote reference module and communication<br />module belong to the middleware.<br />☞ The proxy for B and the skeleton for B represent the<br />so called <em>RMI software</em>. They are situated at the<br />border between middleware and application and<br />usually can be generated automatically with help of<br />available tools that are delivered together with the<br />middleware software.<br />Distributed Systems Fö 3 &#8211; 16<br />Petru Eles, IDA, LiTH<br /><strong>The History of an RMI</strong><br />1. The calling sequence in the client object activates<br />the method in the proxy corresponding to the<br />invoked method in B.<br />2. The method in the proxy packs the arguments into<br />a message (marshalling) and forwards it to the<br />communication module.<br />3. Based on the remote reference obtained from the<br />remote reference module, the communication<br />module initiates the request/reply protocol over the<br />network.<br />4. The communication module on the server’s<br />machine receives the request. Based on the local<br />reference received from the remote reference<br />module the corresponding method in the skeleton<br />for B is activated.<br />5. The skeleton method extracts the arguments from<br />the received message (unmarshalling) and activates<br />the corresponding method in the server object B.<br />6. After receiving the results from B, the method in the<br />skeleton packs them into the message to be sent<br />back (marshalling) and forwards this message to<br />the communication module.<br />7. The communication module sends the reply,<br />through the network, to the client’s machine.<br />8. The communication module receives the reply and<br />forwards it to the corresponding method in the proxy.<br />9. The proxy method extracts the results from the<br />received message (unmarshalling) and forwards<br />them to the client.<br />Distributed Systems Fö 3 &#8211; 17<br />Petru Eles, IDA, LiTH<br /><strong>Remote Procedure Call</strong><br />Request<br />Reply<br />Client stub Sever stub<br />Messages over network<br />Remote reference<br />module<br />Communication<br />module<br />Remote reference<br />module<br />Communication<br />module<br />Marshal<br />arguments<br />Unmarsh.<br />results<br />Client<br />Call<br />Return<br />Server<br />Call<br />Unmarsh.<br />arguments<br />Return<br />Marshal<br />results<br />Distributed Systems Fö 3 &#8211; 18<br />Petru Eles, IDA, LiTH<br /><strong>RMI Semantics and Failures</strong><br />• If everything works OK, RMI behaves exactly like a<br />local invocation. <em>What if certain failures occur</em>?<br />We consider the following classes of failures which have<br />to be handled by an RMI protocol:<br />1. Lost request message<br />2. Lost reply message<br />3. Server crash<br />4. Client crash<br />☞ We will consider an <em>omission failure</em> model.<br />This means:<br />&#8211; Messages are either lost or received correctly.<br />&#8211; Client or server processes either crash or<br />execute correctly. After crash the server can<br />possibly restart with or without loss of memory.<br />Distributed Systems Fö 3 &#8211; 19<br />Petru Eles, IDA, LiTH<br /><strong>Lost Request Messages</strong><br />• The communication module starts a timer when<br />sending the request;<br />if the timer expires before a reply or acknowledgment<br />comes back, the communication module sends the<br />message again.<br />Problem: what if the request was not truly lost (but, for<br />example, the server is too slow) and the server<br />receives it more than once?<br />• We have to avoid that the server executes certain<br />operations more than once.<br />• Messages have to be identified by an identifier and<br />copies of the same message have to be filtered out:<br />&#8211; If the duplicate arrives and the server has not<br />yet sent the reply ⇒ simply send the reply.<br />&#8211; If the duplicate arrives after the reply has been<br />sent ⇒ the reply may have been lost or it didn’t<br />arrive in time (see next slide).<br />Distributed Systems Fö 3 &#8211; 20<br />Petru Eles, IDA, LiTH<br /><strong>Lost Reply Message</strong><br />The client can not really distinguish the loss of a request<br />from that of a reply; it simply resends the request<br />because no answer has been received in the right time.<br />☞ If the reply really got lost, when the duplicate request<br />arrives at the server it already has executed the<br />operation once!<br />☞ In order to resend the reply the server may need to<br />reexecute the operation in order to get the result.<br /><strong>Danger</strong>!<br />• Some operations can be executed more than once<br />without any problem; they are called <em>idempotent</em><br /><em>operations</em> ⇒ no danger with executing the<br />duplicate request.<br />• There are operations which cannot be executed<br />repeatedly without changing the effect (e.g.<br />transferring an amount of money between two<br />accounts) ⇒ <em>history</em> can be used to avoid reexecution.<br />History: the history is a structure which stores a record<br />of reply messages that have been transmitted,<br />together with the message identifier and the<br />client which it has been sent to.<br />Distributed Systems Fö 3 &#8211; 21<br />Petru Eles, IDA, LiTH<br /><strong>Conclusion with Lost Messages</strong><br />☞ Based on the previous discussion ⇒ correct, exactly<br />once semantics (see slide 24) can be implemented in<br />the case of lost (request or reply) messages. <em>If all the</em><br /><em>measures are taken</em> (duplicate filtering and history):<br />&#8211; When, finally, a reply arrives at the client, the<br />call has been executed correctly (exactly one<br />time).<br />&#8211; If no answer arrives at the client (e.g. because<br />of broken line), an operation has been executed<br />at most one time.<br />☞ However, the situation is different if we assume that<br />the server can crash.<br />Distributed Systems Fö 3 &#8211; 22<br />Petru Eles, IDA, LiTH<br /><strong>Server Crash</strong><br />a) The normal sequence:<br />b) The server crashes after executing the operation but<br />before sending the reply (as result of the crash, the<br />server doesn’t remember that it has executed the<br />operation):<br />c) The server crashes before executing the operation:<br />Server<br />Request<br />Reply<br />Server<br />Request<br />no Reply<br />Server<br />Request<br />no Reply<br />receive<br />execute<br />reply<br />receive<br />execute<br /><strong>crash</strong><br />receive<br /><strong>crash</strong><br />Distributed Systems Fö 3 &#8211; 23<br />Petru Eles, IDA, LiTH<br /><strong>Server Crash (cont’d)</strong><br />Big problem!<br />The client cannot distinguish between cases <em>b</em> and <em>c</em>!<br />However they are very different and should be handled in<br />a different way!<br />What to do if the client noticed that the server is down<br />(it didn’t answer to a certain large number of repeated<br />requests)?<br />Distributed Systems Fö 3 &#8211; 24<br />Petru Eles, IDA, LiTH<br /><strong>Server Crash (cont’d)</strong><br />Alternative 1: <em>at least once semantics</em><br />• The client’s communication module sends<br />repeated requests and waits until the server<br />reboots or it is rebound to a new machine; when it<br />finally receives a reply, it forwards it to the client.<br />When the client got an answer, the RMI has been<br />carried out at least one time, but possibly more.<br />Alternative 2: <em>at most once semantics</em><br />• The client’s communication module gives up and<br />immediately reports the failure to the client (e.g. by<br />raising an exception)<br />&#8211; If the client got an answer, the RMI has been<br />executed exactly once.<br />&#8211; If the client got a failure message, the RMI has<br />been carried out at most one time, but possibly<br />not at all.<br />Alternative 3: <em>exactly once semantics</em><br />• This is what we would like to have (and what we<br />could achieve for lost messages): the RMI has<br />been carried out exactly one time.<br /><strong>However this cannot be guaranteed,</strong> <em>in general</em><strong>,</strong><br /><strong>for the situation of server crashes</strong>.<br />Distributed Systems Fö 3 &#8211; 25<br />Petru Eles, IDA, LiTH<br /><strong>Client Crash</strong><br />The client sends a request to a server and crashes<br />before the server replies.<br />The computation which is active in the server becomes<br />an <em>orphan</em> &#8211; a computation nobody is waiting for.<br />Problems:<br />• wasting of CPU time<br />• locked resources (files, peripherals, etc.)<br />• if the client reboots and repeats the RMI, confusion<br />can be created.<br />The solution is based on identification and killing the<br />orphans.<br />Distributed Systems Fö 3 &#8211; 26<br />Petru Eles, IDA, LiTH<br /><strong>Conclusion with RMI Semantics and Failures</strong><br />☞ If the problem of errors is ignored, <em>maybe semantics</em><br />is achieved for RMI:<br />&#8211; the client, in general, doesn’t know if the remote<br />method has been executed once, several times<br />or not at all.<br />☞ If server crashes can be excluded, <em>exactly once</em><br /><em>semantics</em> is possible to achieve, by using retries,<br />filtering out duplicates, and using history.<br />☞ If server crashes with loss of memory (case b on<br />slide 22) are considered, only <em>at least once</em> and <em>at</em><br /><em>most once</em> semantics are achievable in the best case.<br />In practical applications, <em>servers can survive crashes</em><br /><em>without loss of memory</em>. In such cases history can be<br />used and duplicates can be filtered out after restart of<br />the server:<br />• the client repeats sending requests without being in<br />danger operations to be executed more than one<br />time (this is different from alternative 2 on slide 24):<br />&#8211; If no answer is received after a certain amount<br />of tries, the client is notified and he knows that<br />the method has been executed at most one<br />time or not at all.<br />&#8211; If an answer is received it is forwarded to the<br />client who knows that the method has been<br />executed exactly one time.<br />Distributed Systems Fö 3 &#8211; 27<br />Petru Eles, IDA, LiTH<br /><strong>Conclusion with RMI Semantics and Failures (cont’d)</strong><br />☞ RMI semantics is different in different systems.<br />Sometimes several semantics are implemented<br />among which the user is allowed to select.<br />☞ And no hope about achieving exactly once semantics<br />if servers crash ?!<br />In practice, systems can come close to this goal. Such<br />are transaction-based systems with sophisticated<br />protocols for error recovery.<br />☞ More discussion in chapter on fault tolerance.<br />Distributed Systems Fö 3 &#8211; 28<br />Petru Eles, IDA, LiTH<br /><strong>Indirect Communication</strong><br />☞ The communication primitives studied so far are<br />based on <em>direct coupling</em> between sender and<br />receiver; The sender has a reference/pointer to the<br />receiver and specifies it as an argument of the<br />communication primitive.<br />The sender writes something like:<br />&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8211;<br />send (request) to server_reference;<br />&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;<br />Rigidity in dealing with changes<br />Distributed Systems Fö 3 &#8211; 29<br />Petru Eles, IDA, LiTH<br /><strong>Indirect Communication (cont’d)</strong><br />☞ An alternative: <em>Indirect communication</em><br />• No direct coupling between sender and receiver(s).<br />• Communication is performed via an intermediary.<br />• Space uncoupling: sender does not need to know<br />the identity of receiver(s).<br />• Time uncoupling: sender and receiver(s) have<br />independent lifetimes: they do not need to exist at<br />the same time.<br />We look at two examples:<br />1. Group communication<br />2. Publish-subscribe systems<br />Distributed Systems Fö 3 &#8211; 30<br />Petru Eles, IDA, LiTH<br /><strong>Group Communication</strong><br />☞ The assumption with client-server communication<br />and RMI (RPC) is that two parties are involved: the<br />client and the server.<br />☞ Sometimes, however, communication involves<br />multiple processes, not only two.<br />A solution is to perform separate message passing<br />operations or RMIs to each receiver.<br />• With <em>group communication</em> a message can be sent<br />to a group and then it is delivered to all members of<br />the group ⇒ multiple receivers in one operation.<br />Px<br />P4<br />P3<br />P2<br />P1<br />The group<br />send to group<br />group address<br />expansion<br />Distributed Systems Fö 3 &#8211; 31<br />Petru Eles, IDA, LiTH<br /><strong>Group Communication (cont’d)</strong><br />Why do we need it?<br />• Special applications: interest-groups, mail-lists, etc.<br />• Fault tolerance based on replication: a request is<br />sent to <em>several</em> servers which all execute the same<br />operation (if one fails, the client still will be served).<br />• Locating a service or object in a distributed system:<br />the client sends a message to all machines but only<br />the one (or those) which holds the server/object<br />responds.<br />• Replicated data (for reliability or performance):<br />whenever the data changes, the new value has to<br />be sent to all processes managing replicas.<br />Distributed Systems Fö 3 &#8211; 32<br />Petru Eles, IDA, LiTH<br /><strong>Group Communication (cont’d)</strong><br />Group membership management: maintains the view of<br />group membership, considering members joining,<br />leaving, or failing.<br />Services provided by group membership management:<br />• Group membership changes:<br />&#8211; create/destroy process groups;<br />&#8211; add/withdraw processes to/from group.<br />• Failure detection:<br />&#8211; Detects processes that crash or become<br />unavailable (due to e.g. communication failure);<br />&#8211; Excludes processes from membership if they<br />crash or become unavailable.<br />• Notification:<br />&#8211; Notifies members of events e.g. new processes<br />joining or leaving the group.<br />• Group address expansion:<br />&#8211; Processes sending to the group only specify<br />the group identifier; address expansion<br />provides the actual addresses for the multicast<br />operation actually delivering the message to the<br />group members (see slide 30).<br />Distributed Systems Fö 3 &#8211; 33<br />Petru Eles, IDA, LiTH<br /><strong>Group Communication (cont’d)</strong><br />Essential features:<br />• <strong>Atomicity</strong> (all-or-nothing property): when a<br />message is sent to a group, it will either arrive<br />correctly at all members of the group or at none of<br />them.<br />• <strong>Ordering</strong><br />&#8211; FIFO ordering: The messages originating from<br />a given sender are delivered in the same order<br />they have been sent, to all members of the<br />group.<br />&#8211; Total-ordering: When several messages, from<br />different senders, are sent to a group, the<br />messages reach all the members of the group<br />in the same order.<br />Either each process<br />receives the messages<br />in the order <em>m1</em>, <em>m2</em> or<br />each receives them in<br />the order <em>m2</em>, <em>m1</em>.<br />Px<br />P4<br />P3<br />P2<br />P1<br />send <em>m1</em> to group<br />send <em>m2</em> to group<br />Py<br />Distributed Systems Fö 3 &#8211; 34<br />Petru Eles, IDA, LiTH<br /><strong>Publish-Subscribe Systems</strong><br />☞ The general objective of publish-subscribe systems is to<br />let information propagate from publishers to interested<br />subscribers, in an anonymous, decoupled fashion.<br />• <em>Publishers</em> publish events<br />• <em>Subscribers</em> subscribe to and receive the events they<br />are interested in.<br />• Subscribers are not directly targeted from publishers<br />but indirectly via the <em>notification service</em>.<br />• Subscribers express their interest by issuing<br />subscriptions for specific notifications, independently<br />from the publishers that produces them; they are<br />asynchronously notified for all notifications, submitted<br />by any publisher, that match their subscription.<br />publishers<br />S3<br />S2<br />S1<br />Notification<br />service<br />subscribers<br />P4<br />P3<br />P2<br />P1 pub<br />lish(&#8230;)<br />publish(&#8230;)<br />publish(&#8230;)<br />publish(&#8230;)<br />notify(&#8230;)<br />notify(&#8230;)<br />notify(&#8230;)<br />subscribe(&#8230;)<br />unsubscribe(&#8230;)<br />Distributed Systems Fö 3 &#8211; 35<br />Petru Eles, IDA, LiTH<br /><strong>Publish-Subscribe Systems (cont’d)</strong><br />☞ Notification Service: is a propagation mechanism that<br />acts as a logical intermediary between publishers and<br />subscribers, to avoid each publisher to have to know all<br />the subscriptions for each possible subscriber.<br />• Both publishers and subscribers communicate only<br />with a single entity, the notification service, that<br />&#8211; stores all the subscriptions associated with the<br />respective subscribers;<br />&#8211; receives all the notifications from publishers;<br />&#8211; dispatches the notifications to the correct<br />subscribers.<br />☞ A subscription is respectively installed and removed on<br />the notification service as result of subscriber<br />processes executing the <em>subscribe()</em> and <em>unsubscribe()</em><br />operations.<br />☞ A publisher submits a piece of information by executing<br />the <em>publish()</em> operation on the notification service. The<br />notification service dispatches a piece of information to<br />a subscriber by executing the <em>notify()</em> on it.<br />A publisher produces an event (publication), while the<br />notification service issues the corresponding<br />notification on interested subscribers.<br />Distributed Systems Fö 3 &#8211; 36<br />Petru Eles, IDA, LiTH<br /><strong>Publish-Subscribe Systems (cont’d)</strong><br />Example: stock trading system:<br />1. S1 has subscribed to ’IBM’, with a filter indicating<br />that it should be notified only if the stock increases<br />by at least 25; S2 and S3 have subscribed to ’IBM’<br />and ’GM’ respectively, without filter.<br />2. P1 is publishing the new value of ’IBM’.<br />3. S1 is not notified because its filter is not satisfied;<br />S2 is not notified because it’s not interested in<br />’IBM’; S3 is notified.<br />S3<br />S2<br />S1<br />P1 publish(’IBM’, 95)<br />subscribe(’IBM’)<br />BMW<br />11 90<br />GM<br />91<br />IBM<br />subscribe(’IBM’,change>25)<br />notify(’IBM’,95)<br />subscribe(’GM’)<br />Distributed Systems Fö 3 &#8211; 37<br />Petru Eles, IDA, LiTH<br /><strong>Publish-Subscribe Systems (cont’d)</strong><br />☞ One of the main problems with publish-subscribe<br />systems is to achieve scalability of the notification<br />service.<br />• Centralized implementations: are the simplest,<br />however, scalability is limited by the processing<br />power of the machine that hosts the service.<br />• Distributed implementations: the notification<br />service is realised as a network of distributed<br />processes, called brokers; the brokers interact<br />among themselves with the common aim of<br />dispatching notifications to all interested<br />subscribers.<br />&#8211; Such a solution is scalable but is more<br />challenging to implement; it requires complex<br />protocols for the coordination of the various<br />brokers and the diffusion of the information.<br />Distributed Systems Fö 3 &#8211; 38<br />Petru Eles, IDA, LiTH<br /><strong>Summary</strong><br />• Middleware implements high level communication<br />under the form of Remote Method Invocation (RMI)<br />or Remote Procedure Call (RPC). They are based<br />on request/reply protocols which are implemented<br />using message passing on top of a network<br />protocol (like the Internet).<br />• Client-server is a very frequently used<br />communication pattern based on a request/reply<br />protocol; it can be implemented using <em>send</em>/<em>receive</em><br />message passing primitives.<br />• RMI and RPC are elegant mechanisms to<br />implement client-server systems. Remote access is<br />solved like a local one.<br />• Basic components to implement RMI are: the proxy<br />object, the skeleton object, the communication<br />module and the remote reference module.<br />• An essential aspect is RMI semantics in the<br />presence of failures. The goal is to provide <em>exactly</em><br /><em>once</em> semantics. This cannot be achieved, in<br />general, in the presence of server crashes.<br />• Client-server communication, in particular RMI,<br />involves exactly two parties. With group<br />communication a message can be sent to multiple<br />receivers.<br />• Indirect communication, as group communication<br />and publish-subscribe systems, achieves<br />decoupling between sender and receiver(s).<br /></span><br class="Apple-interchange-newline" />