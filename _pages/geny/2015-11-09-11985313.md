---
id: 69
title: 'COM 419 Part 4 &#8211; Memory Hierarchy Design'
date: 2015-11-09T16:21:00+00:00
author: chito
layout: post
guid: http://www.afriqueunique.org/2015/11/09/11985313/
permalink: /2015/11/09/11985313/
swp_pinterest_image_url:
  - ""
swp_cache_timestamp:
  - "419229"
post_views_count:
  - "89"
bs_social_share_facebook:
  - "0"
bs_social_share_twitter:
  - "0"
bs_social_share_reddit:
  - "0"
bs_social_share_google_plus:
  - "0"
bs_social_share_linkedin:
  - "0"
bs_social_share_interval:
  - "1568130618"
categories:
  - LEARNING
---
<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">Memory Hierarchy Design&nbsp;</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">Basics of Memory Hierarchies</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">The increasing size and thus importance of this gap led to the migration of the basics of memory hierarchy into undergraduate courses in computer architecture, and even to courses in operating systems and compilers. Thus, we’ll start with a quick review of caches and their operation. We will describe more advanced innovations that attack the processor–memory performance gap.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">When a word is not found in the cache, the word must be fetched from a lower level in the hierarchy (which may be another cache or the main memory) and placed in the cache before continuing. Multiple words, called a block (or line), are moved for efficiency reasons, and because they are likely to be needed soon due to spatial locality. Each cache block includes a tag to indicate which memory address it corresponds to.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">A key design decision is where blocks (or lines) can be placed in a cache. The most popular scheme is set associative, where a set is a group of blocks in the cache. A block is first mapped onto a set, and then the block can be placed anywhere within that set. Finding a block consists of first mapping the block address to the set and then searching the set—usually in parallel—to find the block. The set is chosen by the address of the data:</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">(Block address) MOD (Number of sets in cache)</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">If there are n blocks in a set, the cache placement is called n-way set associative.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">The end points of set associativity have their own names. A direct-mapped cache has just one block per set (so a block is always placed in the same location), and a fully associative cache has just one set (so a block can be placed anywhere).</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">Caching data that is only read is easy, since the copy in the cache and memory will be identical. Caching writes is more difficult; for example, how can the copy in the cache and memory be kept consistent? There are two main strategies.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">A write-through cache updates the item in the cache and writes through to update main memory. A write-back cache only updates the copy in the cache. When the block is about to be replaced, it is copied back to memory. Both write strategies can use a write buffer to allow the cache to proceed as soon as the data are placed in the buffer rather than wait the full latency to write the data into memory.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">One measure of the benefits of different cache organizations is miss rate. Miss rate is simply the fraction of cache accesses that result in a miss—that is, the number of accesses that miss divided by the number of accesses. To gain insights into the causes of high miss rates, which can inspire better cache designs, the three Cs model sorts all misses into three simple categories:</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">■ Compulsory: The very first access to a block cannot be in the cache, so the block must be brought into the cache. Compulsory misses are those that occur even if you had an infinite sized cache.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">■ Capacity: If the cache cannot contain all the blocks needed during execution of a program, capacity misses (in addition to compulsory misses) will occur because of blocks being discarded and later retrieved.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">■ Conflict: If the block placement strategy is not fully associative, conflict misses (in addition to compulsory and capacity misses) will occur because a block may be discarded and later retrieved if multiple blocks map to its set and accesses to the different blocks are intermingled.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">&nbsp;</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">Multithreading and multiple cores add complications for caches, both increasing the potential for capacity misses as well as adding a fourth C, for coherency misses due to cache flushes to keep multiple caches coherent in a multiprocessor.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">Miss rate can be a misleading measure for several reasons. Hence, some designers prefer measuring misses per instruction rather than misses per memory reference (miss rate).</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">The problem with both measures is that they dont factor in the cost of a miss.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">A better measure is the average memory access time:</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">Average memory access time = Hit time + Miss rate × Miss penalty</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">where hit time is the time to hit in the cache and miss penalty is the time to replace the block from memory (that is, the cost of a miss). Average memory access time is still an indirect measure of performance; although it is a better measure than miss rate, it is not a substitute for execution time.&nbsp;</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">Speculative processors may execute other instructions during a miss, thereby reducing the effective miss penalty.&nbsp;</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">The use of multithreading also allows a processor to tolerate missses without being forced to idle. As we will examine shortly, to take advantage of such latency tolerating techniques we need caches that can service requests while handling an outstanding miss.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">We present six basic cache optimizations and also give quantitative examples of the benefits of these optimizations. We also comment briefly on the power implications of these trade-offs.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">1. Larger block size to reduce miss rate: The simplest way to reduce the miss rate is to take advantage of spatial locality and increase the block size. Larger blocks reduce compulsory misses, but they also increase the miss penalty.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">Because larger blocks lower the number of tags, they can slightly reduce static power. Larger block sizes can also increase capacity or conflict misses, especially in smaller caches. Choosing the right block size is a complex trade-off that depends on the size of cache and the miss penalty.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">2. Bigger caches to reduce miss rate: The obvious way to reduce capacity misses is to increase cache capacity. Drawbacks include potentially longer hit time of the larger cache memory and higher cost and power. Larger caches increase both static and dynamic power.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">3. Higher associativity to reduce miss rate: Obviously, increasing associativity reduces conflict misses. Greater associativity can come at the cost of increased hit time. As we will see shortly, associativity also increases power consumption.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">4. Multilevel caches to reduce miss penalty: A difficult decision is whether to make the cache hit time fast, to keep pace with the high clock rate of processors, or to make the cache large to reduce the gap between the processor accesses and main memory accesses. Adding another level of cache between the original cache and memory simplifies the decision. The first-level cache can be small enough to match a fast clock cycle time, yet the second-level (or third-level) cache can be large enough to capture many accesses that would go to main memory. The focus on misses in second-level caches leads to larger blocks, bigger capacity, and higher associativity. Multilevel caches are more power efficient than a single aggregate cache. If L1 and L2 refer, respectively, to first- and second-level caches, we can redefine the average memory access time:</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">Hit timeL1 + Miss rateL1 × (Hit timeL2 + Miss rateL2 × Miss penaltyL2)</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">5. Giving priority to read misses over writes to reduce miss penalty: A write buffer is a good place to implement this optimization. Write buffers create hazards because they hold the updated value of a location needed on a read miss—that is, a read-after-write hazard through memory. One solution is to check the contents of the write buffer on a read miss. If there are no conflicts, and if the memory system is available, sending the read before the writes reduces the miss penalty. Most processors give reads priority over writes. This choice has little effect on power consumption.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">6. Avoiding address translation during indexing of the cache to reduce hit time: Caches must cope with the translation of a virtual address from the processor to a physical address to access memory.&nbsp;</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">A common optimization is to use the page offset—the part that is identical in both virtual and physical addresses—to index the cache. This virtual index/physical tag method introduces some system complications and/or limitations on the size and structure of the L1 cache, but the advantages of removing the translation lookaside buffer (TLB) access from the critical path outweigh the disadvantages.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">&nbsp;</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">Note that each of the six optimizations above has a potential disadvantage that can lead to increased, rather than decreased, average memory access time.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">In the Putting It All Together, we examine the memory hierarchy for a microprocessor designed for a high-end server, the Intel Core i7, as well as one designed for use in a PMD, the Arm Cortex-A8, which is the basis for the processor used in the Apple iPad and several high-end smartphones. Within each of these classes, there is a significant diversity in approach due to the intended use of the computer. While the high-end processor used in the server has more cores and bigger caches than the Intel processors designed for desktop uses, the processors have similar architectures. The differences are driven by performance and the nature of the workload; desktop computers are primarily running one application at a time on top of an operating system for a single user, whereas server computers may have hundreds of users running potentially dozens of applications simultaneously. Because of these workload differences, desktop computers are generally concerned more with average latency from the memory hierarchy, whereas server computers are also concerned about memory bandwidth. Even within the class of desktop computers there is wide diversity from lower end netbooks with scaled-down processors more similar to those found in high-end PMDs, to high-end desktops whose processors contain multiple cores and whose organization resembles that of a low-end server.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">In contrast, PMDs not only serve one user but generally also have smaller operating systems, usually less multitasking (running of several applications simultaneously), and simpler applications. PMDs also typically use Flash memory rather than disks, and most consider both performance and energy consumption, which determines battery life.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">&nbsp;</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">Ten Advanced Optimizations of Cache Performance</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">The average memory access time formula gives us three metrics for cache optimizations:&nbsp;</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">§ hit time,&nbsp;</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">§ miss rate, and&nbsp;</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">§ miss penalty.&nbsp;</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">&nbsp;</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">Given the recent trends, we add cache bandwidth and power consumption to this list. We can classify the ten advanced cache optimizations we examine into five categories based on these metrics:</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">1. Reducing the hit time: Small and simple first-level caches and wayprediction. Both techniques also generally decrease power consumption. &nbsp;</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">2. Increasing cache bandwidth: Pipelined caches, multibanked caches, and nonblocking caches. These techniques have varying impacts on power consumption.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">3. Reducing the miss penalty: Critical word first and merging write buffers.&nbsp;</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">&nbsp;</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">These optimizations have little impact on power.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">4. Reducing the miss rate—Compiler optimizations. Obviously any improvement at compile time improves power consumption.</span>
</p>

<p class="p0" style="margin-bottom:0pt;margin-top:0pt;">
  <span style="font-weight:bold;line-height:18.2399997711182px;font-size:small;">5. Reducing the miss penalty or miss rate via parallelism: Hardware prefetching and compiler prefetching. These optimizations generally increase power consumption, primarily due to prefetched data that are unused.</span>
</p>